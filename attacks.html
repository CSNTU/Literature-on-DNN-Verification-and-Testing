<head>
  <meta charset="utf-8">

  <title>DNN Verification and Testing: Attacking Feedforward DNNs </title>
  <meta name="description" content="DNN Verification and Testing: Attacking Techniques">
  <meta name="author" content="SitePoint">

  <link rel="stylesheet" href="css/styles.css?v=1.0">

  <!--[if lt IE 9]>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.js"></script>
  <![endif]-->
</head>

<body>
  <script src="js/scripts.js"></script>
  
  <h2>DNN Verification and Testing: Attacking Feedforward DNNs </h2>
  
<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-yw4l{vertical-align:top}
</style>
<table class="tg">
  <tr>
    <th class="tg-yw4l"> Title </th> 
    <th> Link </th>    
    <th class="tg-yw4l"> Comment </th> 
  </tr>
  
  <tr>
    <th class="tg-yw4l"> Explaining and Harnessing Adversarial Examples </th> 
    <th> <a href="https://arxiv.org/abs/1412.6572">link</a> </th>  
    <th class="tg-yw4l">  </th>   
  </tr>
  
  <tr>      
    <th class="tg-yw4l"> DeepFool: a simple and accurate method to fool deep neural networks </th> 
    <th> <a href="https://arxiv.org/abs/1511.04599">link</a> </th> 
    <th class="tg-yw4l">  </th> 
  </tr>
  
  <tr>      
    <th class="tg-yw4l"> The Limitations of Deep Learning in Adversarial Settings </th> 
    <th> <a href="https://arxiv.org/abs/1511.07528">link</a> </th> 
    <th class="tg-yw4l">  </th> 
  </tr>
  
  <tr>      
    <th class="tg-yw4l"> Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images </th> 
    <th> <a href="https://arxiv.org/abs/1412.1897">link</a> </th> 
    <th class="tg-yw4l">  </th> 
  </tr>
  
  <tr>      
    <th class="tg-yw4l"> Towards Evaluating the Robustness of Neural Networks </th> 
    <th> <a href="https://arxiv.org/abs/1608.04644">link</a> </th> 
    <th class="tg-yw4l">  </th> 
  </tr>
  
  <tr>      
    <th class="tg-yw4l"> Adversarial examples in the physical world </th> 
    <th> <a href="https://arxiv.org/abs/1607.02533">link</a> </th> 
    <th class="tg-yw4l">  </th> 
  </tr>
  
  <tr>      
    <th class="tg-yw4l"> Universal adversarial perturbations </th> 
    <th> <a href="https://arxiv.org/abs/1610.08401">link</a> </th> 
    <th class="tg-yw4l">  </th> 
  </tr>
  
  <tr>      
    <th class="tg-yw4l"> Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples </th> 
    <th> <a href="https://arxiv.org/abs/1605.07277">link</a> </th> 
    <th class="tg-yw4l">  </th> 
  </tr>
  
  <tr>      
    <th class="tg-yw4l"> Practical Black-Box Attacks against Machine Learning </th> 
    <th> <a href="https://arxiv.org/abs/1602.02697">link</a> </th> 
    <th class="tg-yw4l">  </th> 
  </tr>
  
  <tr>      
    <th class="tg-yw4l"> Robust Physical-World Attacks on Deep Learning Models </th> 
    <th> <a href="https://arxiv.org/abs/1707.08945">link</a> </th> 
    <th class="tg-yw4l">  </th> 
  </tr>
  
  <tr>      
    <th class="tg-yw4l"> NO Need to Worry about Adversarial Examples in Object Detection in Autonomous Vehicles </th> 
    <th> <a href="https://arxiv.org/abs/1707.03501">link</a> </th> 
    <th class="tg-yw4l">  </th> 
  </tr>
  
</table>
  
</body>
</html>
