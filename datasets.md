<head>
  <meta charset="utf-8">

  <meta name="description" content="DNN Verification and Testing: Datasets">
  <meta name="author" content="SitePoint">

  <link rel="stylesheet" href="css/styles.css?v=1.0">

  <!--[if lt IE 9]>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.js"></script>
  <![endif]-->
</head>

<body>
  
  <h2>DNN Verification and Testing: Datasets</h2>
  
<table class="tg">
  <tr>
    <th class="tg-yw4l"> Title </th> 
    <th> Link </th>    
    <th class="tg-yw4l"> Comment </th> 
  </tr>
  
  <tr>
    <th class="tg-yw4l"> The ApolloScape Dataset for Autonomous Driving </th> 
    <th> <a href="https://arxiv.org/abs/1803.06184">link</a> </th>  
    <th class="tg-yw4l">  </th>   
  </tr>

  <tr>
    <th class="tg-yw4l"> BDD100K: A Diverse Driving Video Database with Scalable Annotation Tooling </th> 
    <th> <a href="https://arxiv.org/abs/1805.04687">link</a> </th>  
    <th class="tg-yw4l">  </th>   
  </tr>

  <tr>
    <th class="tg-yw4l"> WILDTRACK: A Multi-Camera HD Dataset for Dense Unscripted Pedestrian Detection </th> 
    <th> <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Chavdarova_WILDTRACK_A_Multi-Camera_CVPR_2018_paper.pdf">link</a> </th>  
    <th class="tg-yw4l"> CVPR2018 </th>   
  </tr>

  <tr>
    <th class="tg-yw4l"> 4DFAB: A Large Scale 4D Database for Facial Expression Analysis and Biometric Applications </th> 
    <th> <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Cheng_4DFAB_A_Large_CVPR_2018_paper.pdf">link</a> </th>  
    <th class="tg-yw4l"> CVPR2018 </th>   
  </tr>

  <tr>
    <th class="tg-yw4l"> Revisiting Oxford and Paris: Large-Scale Image Retrieval Benchmarking </th> 
    <th> <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Radenovic_Revisiting_Oxford_and_CVPR_2018_paper.pdf">link</a> </th>  
    <th class="tg-yw4l"> CVPR2018 </th>   
  </tr>

  <tr>
    <th class="tg-yw4l"> LiDAR-Video Driving Dataset: Learning Driving Policies Effectively </th> 
    <th> <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper.pdf">link</a> </th>  
    <th class="tg-yw4l"> CVPR2018 </th>   
  </tr>

  <tr>
    <th class="tg-yw4l"> AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Actions </th> 
    <th> <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Gu_AVA_A_Video_CVPR_2018_paper.pdf">link</a> </th>  
    <th class="tg-yw4l"> CVPR2018 </th>   
  </tr>

  <tr>
    <th class="tg-yw4l"> Toward Driving Scene Understanding: A Dataset for Learning Driver Behavior and Causal Reasoning </th> 
    <th> <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Ramanishka_Toward_Driving_Scene_CVPR_2018_paper.pdf">link</a> </th>  
    <th class="tg-yw4l"> CVPR2018 </th>   
  </tr>

  <tr>
    <th class="tg-yw4l"> VegFru: A Domain-Specific Dataset for Fine-grained Visual Categorization </th> 
    <th> <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Hou_VegFru_A_Domain-Specific_ICCV_2017_paper.pdf">link</a> </th>  
    <th class="tg-yw4l"> ICCV2017 </th>   
  </tr>

  <tr>
    <th class="tg-yw4l"> Need for Speed: A Benchmark for Higher Frame Rate Object Tracking </th> 
    <th> <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Galoogahi_Need_for_Speed_ICCV_2017_paper.pdf">link</a> </th>  
    <th class="tg-yw4l"> ICCV2017 </th>   
  </tr>

  <tr>
    <th class="tg-yw4l"> Playing for Benchmarks </th> 
    <th> <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Richter_Playing_for_Benchmarks_ICCV_2017_paper.pdf">link</a> </th>  
    <th class="tg-yw4l"> ICCV2017 </th>   
  </tr>

  <tr>
    <th class="tg-yw4l"> TorontoCity: Seeing the World with a Million Eyes </th> 
    <th> <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Wang_TorontoCity_Seeing_the_ICCV_2017_paper.pdf">link</a> </th>  
    <th class="tg-yw4l"> ICCV2017 </th>   
  </tr>

  <tr>
    <th class="tg-yw4l"> MS-Celeb-1M: A Dataset and Benchmark for Large-Scale Face Recognition </th> 
    <th> <a href="https://link.springer.com/content/pdf/10.1007%2F978-3-319-46487-9_6.pdf">link</a> </th>  
    <th class="tg-yw4l"> ECCV2016 </th>   
  </tr>

  <tr>
    <th class="tg-yw4l"> A Large Contextual Dataset for Classification, Detection and Counting of Cars with Deep Learning </th> 
    <th> <a href="https://link.springer.com/content/pdf/10.1007%2F978-3-319-46487-9_48.pdf">link</a> </th>  
    <th class="tg-yw4l"> ECCV2016 </th>   
  </tr>

  <tr>
    <th class="tg-yw4l"> A Benchmark for Automatic Visual Classification of Clinical Skin Disease Images </th> 
    <th> <a href="https://link.springer.com/content/pdf/10.1007%2F978-3-319-46466-4_13.pdf">link</a> </th>  
    <th class="tg-yw4l"> ECCV2016 </th>   
  </tr>
  
</table>

<a href="https://github.com/TrustAI/Literature-on-DNN-Verification-and-Testing">Back</a>
  
</body>
</html>
