<head>
  <meta charset="utf-8">

  <meta name="description" content="DNN Verification and Testing: Attacking Techniques">
  <meta name="author" content="SitePoint">

  <link rel="stylesheet" href="css/styles.css?v=1.0">

  <!--[if lt IE 9]>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.js"></script>
  <![endif]-->
</head>

<body>
  
  <h2>DNN Verification and Testing: Attacking Feedforward DNNs </h2>
  
<table class="tg">
  <tr>
    <th class="tg-yw4l"> Title </th> 
    <th> Link </th>    
    <th class="tg-yw4l"> Comment </th> 
  </tr>
  
  <tr>
    <th class="tg-yw4l"> Explaining and Harnessing Adversarial Examples </th> 
    <th> <a href="https://arxiv.org/abs/1412.6572">link</a> </th>  
    <th class="tg-yw4l">  </th>   
  </tr>
  
  <tr>      
    <th class="tg-yw4l"> DeepFool: a simple and accurate method to fool deep neural networks </th> 
    <th> <a href="https://arxiv.org/abs/1511.04599">link</a> </th> 
    <th class="tg-yw4l">  </th> 
  </tr>
  
  <tr>      
    <th class="tg-yw4l"> The Limitations of Deep Learning in Adversarial Settings </th> 
    <th> <a href="https://arxiv.org/abs/1511.07528">link</a> </th> 
    <th class="tg-yw4l">  </th> 
  </tr>
  
  <tr>      
    <th class="tg-yw4l"> Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images </th> 
    <th> <a href="https://arxiv.org/abs/1412.1897">link</a> </th> 
    <th class="tg-yw4l">  </th> 
  </tr>
  
  <tr>      
    <th class="tg-yw4l"> Towards Evaluating the Robustness of Neural Networks </th> 
    <th> <a href="https://arxiv.org/abs/1608.04644">link</a> </th> 
    <th class="tg-yw4l">  </th> 
  </tr>
  
  <tr>      
    <th class="tg-yw4l"> Adversarial examples in the physical world </th> 
    <th> <a href="https://arxiv.org/abs/1607.02533">link</a> </th> 
    <th class="tg-yw4l">  </th> 
  </tr>
  
  <tr>      
    <th class="tg-yw4l"> Universal adversarial perturbations </th> 
    <th> <a href="https://arxiv.org/abs/1610.08401">link</a> </th> 
    <th class="tg-yw4l">  </th> 
  </tr>
  
  <tr>      
    <th class="tg-yw4l"> Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples </th> 
    <th> <a href="https://arxiv.org/abs/1605.07277">link</a> </th> 
    <th class="tg-yw4l">  </th> 
  </tr>
  
  <tr>      
    <th class="tg-yw4l"> Practical Black-Box Attacks against Machine Learning </th> 
    <th> <a href="https://arxiv.org/abs/1602.02697">link</a> </th> 
    <th class="tg-yw4l">  </th> 
  </tr>
  
  <tr>      
    <th class="tg-yw4l"> Robust Physical-World Attacks on Deep Learning Models </th> 
    <th> <a href="https://arxiv.org/abs/1707.08945">link</a> </th> 
    <th class="tg-yw4l">  </th> 
  </tr>
  
  <tr>      
    <th class="tg-yw4l"> NO Need to Worry about Adversarial Examples in Object Detection in Autonomous Vehicles </th> 
    <th> <a href="https://arxiv.org/abs/1707.03501">link</a> </th> 
    <th class="tg-yw4l">  </th> 
  </tr>
  
  <tr>      
    <th class="tg-yw4l"> Boosting Adversarial Attacks with Momentum </th> 
    <th> <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Dong_Boosting_Adversarial_Attacks_CVPR_2018_paper.pdf">link</a></th> 
    <th class="tg-yw4l">  </th> 
  </tr>
  
  
   <tr>      
    <th class="tg-yw4l"> Art of singular vectors and universal adversarial perturbations </th> 
    <th> <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Khrulkov_Art_of_Singular_CVPR_2018_paper.pdf">link</a> </th> 
    <th class="tg-yw4l">  </th> 
  </tr>
  
   <tr>      
    <th class="tg-yw4l">   Generative Adversarial Perturbations </th> 
    <th> <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Poursaeed_Generative_Adversarial_Perturbations_CVPR_2018_paper.pdf">link</a> </th> 
    <th class="tg-yw4l">  </th> 
  </tr>
  
   <tr>      
    <th class="tg-yw4l">  LaVAN: Localized and Visible Adversarial Noise </th> 
    <th> <a href="https://arxiv.org/pdf/1801.02608.pdf">link</a> </th> 
    <th class="tg-yw4l"> ICML2018 </th> 
  </tr>
  
   <tr>      
    <th class="tg-yw4l">  Synthesizing Robust Adversarial Examples </th> 
    <th> <a href="https://arxiv.org/abs/1707.07397">link</a> </th> 
    <th class="tg-yw4l"> ICML2018 </th> 
  </tr>
  
  
</table>

<a href="https://github.com/TrustAI/Literature-on-DNN-Verification-and-Testing">Back</a>
  
</body>
</html>
