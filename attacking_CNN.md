<head>
  <meta charset="utf-8">

  <meta name="description" content="DNN Verification and Testing: Attacking Techniques">
  <meta name="author" content="SitePoint">

  <link rel="stylesheet" href="css/styles.css?v=1.0">

  <!--[if lt IE 9]>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.js"></script>
  <![endif]-->
</head>

<body>
  
  <h2>DNN Verification and Testing: Attacking Feedforward DNNs </h2>
  
<table class="tg">
  <tr>
    <th class="tg-yw4l"> Title </th> 
    <th> Link </th>    
    <th class="tg-yw4l"> Comment </th> 
  </tr>
  
  <tr>
    <th class="tg-yw4l"> Explaining and Harnessing Adversarial Examples </th> 
    <th> <a href="https://arxiv.org/abs/1412.6572">link</a> </th>  
    <th class="tg-yw4l">  </th>   
  </tr>
  
  <tr>      
    <th class="tg-yw4l"> DeepFool: a simple and accurate method to fool deep neural networks </th> 
    <th> <a href="https://arxiv.org/abs/1511.04599">link</a> </th> 
    <th class="tg-yw4l">  </th> 
  </tr>
  
  <tr>      
    <th class="tg-yw4l"> The Limitations of Deep Learning in Adversarial Settings </th> 
    <th> <a href="https://arxiv.org/abs/1511.07528">link</a> </th> 
    <th class="tg-yw4l">  Euro S&P 2016 </th> 
  </tr>
  
  <tr>      
    <th class="tg-yw4l"> Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images </th> 
    <th> <a href="https://arxiv.org/abs/1412.1897">link</a> </th> 
    <th class="tg-yw4l">  </th> 
  </tr>
  
  <tr>      
    <th class="tg-yw4l"> Towards Evaluating the Robustness of Neural Networks </th> 
    <th> <a href="https://arxiv.org/abs/1608.04644">link</a> </th> 
    <th class="tg-yw4l">  S&P2017 </th> 
  </tr>
  
  <tr>      
    <th class="tg-yw4l"> Adversarial examples in the physical world </th> 
    <th> <a href="https://arxiv.org/abs/1607.02533">link</a> </th> 
    <th class="tg-yw4l">  </th> 
  </tr>
  
  <tr>      
    <th class="tg-yw4l"> Universal adversarial perturbations </th> 
    <th> <a href="https://arxiv.org/abs/1610.08401">link</a> </th> 
    <th class="tg-yw4l">  </th> 
  </tr>
  
  <tr>      
    <th class="tg-yw4l"> Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples </th> 
    <th> <a href="https://arxiv.org/abs/1605.07277">link</a> </th> 
    <th class="tg-yw4l">  </th> 
  </tr>
  
  <tr>      
    <th class="tg-yw4l"> Practical Black-Box Attacks against Machine Learning </th> 
    <th> <a href="https://arxiv.org/abs/1602.02697">link</a> </th> 
    <th class="tg-yw4l">  </th> 
  </tr>
  
  <tr>      
    <th class="tg-yw4l"> Robust Physical-World Attacks on Deep Learning Models </th> 
    <th> <a href="https://arxiv.org/abs/1707.08945">link</a> </th> 
    <th class="tg-yw4l">  </th> 
  </tr>
  
  <tr>      
    <th class="tg-yw4l"> NO Need to Worry about Adversarial Examples in Object Detection in Autonomous Vehicles </th> 
    <th> <a href="https://arxiv.org/abs/1707.03501">link</a> </th> 
    <th class="tg-yw4l">  </th> 
  </tr>
  
  <tr>      
    <th class="tg-yw4l"> Boosting Adversarial Attacks with Momentum </th> 
    <th> <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Dong_Boosting_Adversarial_Attacks_CVPR_2018_paper.pdf">link</a></th> 
    <th class="tg-yw4l">  </th> 
  </tr>
  
  
   <tr>      
    <th class="tg-yw4l"> Art of singular vectors and universal adversarial perturbations </th> 
    <th> <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Khrulkov_Art_of_Singular_CVPR_2018_paper.pdf">link</a> </th> 
    <th class="tg-yw4l">  </th> 
  </tr>
  
   <tr>      
    <th class="tg-yw4l">   Generative Adversarial Perturbations </th> 
    <th> <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Poursaeed_Generative_Adversarial_Perturbations_CVPR_2018_paper.pdf">link</a> </th> 
    <th class="tg-yw4l">  </th> 
  </tr>
  
   <tr>      
    <th class="tg-yw4l">  LaVAN: Localized and Visible Adversarial Noise </th> 
    <th> <a href="https://arxiv.org/pdf/1801.02608.pdf">link</a> </th> 
    <th class="tg-yw4l"> ICML2018 </th> 
  </tr>
  
   <tr>      
    <th class="tg-yw4l">  Synthesizing Robust Adversarial Examples </th> 
    <th> <a href="https://arxiv.org/abs/1707.07397">link</a> </th> 
    <th class="tg-yw4l"> ICML2018 </th> 
  </tr>
  
  
   <tr>      
    <th class="tg-yw4l"> Black-box Adversarial Attacks with Limited Queries and Information </th> 
    <th> <a href="https://arxiv.org/abs/1804.08598">link</a> </th> 
    <th class="tg-yw4l"> ICML2018 </th> 
  </tr>
  
   <tr>      
    <th class="tg-yw4l"> Robustness of classifiers: from adversarial to random noise </th> 
    <th> <a href="http://papers.nips.cc/paper/6331-robustness-of-classifiers-from-adversarial-to-random-noise">link</a> </th> 
    <th class="tg-yw4l"> NIPS2016 </th> 
  </tr>
  
   <tr>      
    <th class="tg-yw4l"> Attacking Binarized Neural Networks </th> 
    <th> <a href="https://arxiv.org/abs/1711.00449">link</a> </th> 
    <th class="tg-yw4l"> ICLR2018 </th> 
  </tr>
  
   <tr>      
    <th class="tg-yw4l">Decision-Based Adversarial Attacks- Reliable Attacks Against Black-Box Machine Learning Models</th> 
    <th> <a href="https://arxiv.org/abs/1712.04248">link</a> </th> 
    <th class="tg-yw4l"> ICLR2018 </th> 
  </tr>
  
   <tr>      
    <th class="tg-yw4l">Generating Natural Adversarial Examples</th> 
    <th> <a href="https://arxiv.org/abs/1710.11342">link</a> </th> 
    <th class="tg-yw4l"> ICLR2018 </th> 
  </tr>
  
   <tr>      
    <th class="tg-yw4l">Spatially Transformed Adversarial Examples</th> 
    <th> <a href="https://arxiv.org/abs/1801.02612">link</a> </th> 
    <th class="tg-yw4l"> ICLR2018 </th> 
  </tr>
  
   <tr>      
    <th class="tg-yw4l">A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks</th> 
    <th> <a href="https://arxiv.org/abs/1610.02136">link</a> </th> 
    <th class="tg-yw4l"> ICLR2017 </th> 
  </tr>
  
   <tr>      
    <th class="tg-yw4l">Adversarial examples in the physical world</th> 
    <th> <a href="https://arxiv.org/abs/1607.02533">link</a> </th> 
    <th class="tg-yw4l"> ICLR2017 </th> 
  </tr>
  
   <tr>      
    <th class="tg-yw4l">Delving into Transferable Adversarial Examples and Black-box Attacks</th> 
    <th> <a href="https://arxiv.org/abs/1611.02770">link</a> </th> 
    <th class="tg-yw4l"> ICLR2017 </th> 
  </tr>
  
   <tr>      
    <th class="tg-yw4l">Adversarial Manipulation of Deep Representations</th> 
    <th> <a href="https://arxiv.org/abs/1511.05122">link</a> </th> 
    <th class="tg-yw4l"> ICLR2016 </th> 
  </tr>
  
   <tr>      
    <th class="tg-yw4l">Explaining and Harnessing Adversarial Examples</th> 
    <th> <a href="https://arxiv.org/abs/1412.6572">link</a> </th> 
    <th class="tg-yw4l"> ICLR2015 </th> 
  </tr>
  
   <tr>      
    <th class="tg-yw4l">Intriguing properties of neural networks</th> 
    <th> <a href="https://arxiv.org/abs/1312.6199">link</a> </th> 
    <th class="tg-yw4l"> ICLR2014 </th> 
  </tr>

   <tr>      
    <th class="tg-yw4l">NAG: Network for Adversary Generation</th> 
    <th> <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Mopuri_NAG_Network_for_CVPR_2018_paper.pdf">link</a> </th> 
    <th class="tg-yw4l"> CVPR2018 </th> 
  </tr>

   <tr>      
    <th class="tg-yw4l">Generative Adversarial Perturbations</th> 
    <th> <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Poursaeed_Generative_Adversarial_Perturbations_CVPR_2018_paper.pdf">link</a> </th> 
    <th class="tg-yw4l"> CVPR2018 </th> 
  </tr>

   <tr>      
    <th class="tg-yw4l">Boosting Adversarial Attacks with Momentum</th> 
    <th> <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Dong_Boosting_Adversarial_Attacks_CVPR_2018_paper.pdf">link</a> </th> 
    <th class="tg-yw4l"> CVPR2018 </th> 
  </tr>

   <tr>      
    <th class="tg-yw4l">Adversarial Examples for Semantic Segmentation and Object Detection</th> 
    <th> <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Xie_Adversarial_Examples_for_ICCV_2017_paper.pdf">link</a> </th> 
    <th class="tg-yw4l"> ICCV2017 </th> 
  </tr>
  
</table>

<a href="https://github.com/TrustAI/Literature-on-DNN-Verification-and-Testing">Back</a>
  
</body>
</html>
