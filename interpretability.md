<head>
  <meta charset="utf-8">

  <meta name="description" content="DNN Verification and Testing: Attacking Techniques">
  <meta name="author" content="SitePoint">

  <link rel="stylesheet" href="css/styles.css?v=1.0">

  <!--[if lt IE 9]>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.js"></script>
  <![endif]-->
</head>

<body>
  
  <h2>DNN Verification and Testing: Interpretability </h2>
  
<table class="tg">

  <tr>
    <th class="tg-yw4l"> Title </th> 
    <th> Link </th>    
    <th class="tg-yw4l"> Comment </th> 
  </tr>
  
  <tr>
    <th class="tg-yw4l"> The Mythos of Model Interpretability </th> 
    <th> <a href="https://arxiv.org/abs/1606.03490">link</a> </th>    
    <th class="tg-yw4l">  </th>   
  </tr>
  
  <tr>
    <th class="tg-yw4l"> "Why Should I Trust You?": Explaining the Predictions of Any Classifier </th> 
    <th> <a href="https://arxiv.org/abs/1602.04938">link</a> </th>    
    <th class="tg-yw4l"> LIME  </th>   
  </tr>
  
  <tr>
    <th class="tg-yw4l"> Rationalizing Neural Predictions </th> 
    <th> <a href="https://arxiv.org/abs/1606.04155">link</a> </th>    
    <th class="tg-yw4l">  </th>   
  </tr>
  
  <tr>
    <th class="tg-yw4l"> Interpretable Explanations of Black Boxes by Meaningful Perturbation </th> 
    <th> <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Fong_Interpretable_Explanations_of_ICCV_2017_paper.pdf">link</a> </th>    
    <th class="tg-yw4l">   </th>   
  </tr>
  
  <tr>
    <th class="tg-yw4l"> Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization </th> 
    <th> <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Selvaraju_Grad-CAM_Visual_Explanations_ICCV_2017_paper.pdf">link</a> </th>    
    <th class="tg-yw4l">   </th>   
  </tr>
  
  <tr>
    <th class="tg-yw4l"> Understanding Black-box Predictions via Influence Functions </th> 
    <th> <a href="https://arxiv.org/abs/1703.04730">link</a> </th>    
    <th class="tg-yw4l">  </th>   
  </tr>
  
  <tr>
    <th class="tg-yw4l"> Axiomatic Attribution for Deep Networks </th> 
    <th> <a href="https://arxiv.org/abs/1703.01365">link</a> </th>    
    <th class="tg-yw4l">  </th>   
  </tr>
  
  <tr>
    <th class="tg-yw4l"> A Unified Approach to Interpreting Model Predictions </th> 
    <th> <a href="https://arxiv.org/abs/1705.07874">link</a> </th>    
    <th class="tg-yw4l">  </th>   
  </tr>
    
  <tr>
    <th class="tg-yw4l"> Explanation and Justification in Machine Learning: A Survey </th> 
    <th> <a href="http://home.earthlink.net/~dwaha/research/meetings/ijcai17-xai/1.%20(Biran%20&%20Cotton%20XAI-17)%20Explanation%20and%20Justification%20in%20ML%20-%20A%20Survey.pdf">link</a> </th>    
    <th class="tg-yw4l">  </th>   
  </tr>
  
  <tr>
    <th class="tg-yw4l"> Using Explanations to Improve Ensembling of Visual Question Answering Systems </th> 
    <th> <a href="http://home.earthlink.net/~dwaha/research/meetings/ijcai17-xai/7.%20(Rajani%20&%20Mooney%20XAI-17)%20Using%20Explanations%20to%20Improve%20Ensembling%20of%20VQA%20Systems.pdf">link</a> </th>    
    <th class="tg-yw4l">  </th>   
  </tr>
    
</table>

<a href="https://github.com/TrustAI/Literature-on-DNN-Verification-and-Testing">Back</a>
  
</body>
</html>
